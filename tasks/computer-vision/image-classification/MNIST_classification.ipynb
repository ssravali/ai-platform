{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MNIST_classification.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "jpv1njm-34kT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "7151bb67-4747-4aa1-fecb-432c4a153359"
      },
      "source": [
        "#Libraries\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Conv2D, Dropout, Flatten, MaxPooling2D\n",
        "from keras.callbacks import EarlyStopping\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "# Loading dataset\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
        "# Reshaping the array to 4-dims so that it can work with the Keras API\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "input_shape = (28, 28, 1)\n",
        "# Making sure that the values are float so that we can get decimal points after division\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "# Normalizing the RGB codes by dividing it to the max RGB value.\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('Number of images in x_train', x_train.shape[0])\n",
        "print('Number of images in x_test', x_test.shape[0])\n",
        "# Creating a Sequential Model and adding the layers\n",
        "model = Sequential()\n",
        "model.add(Conv2D(28, kernel_size=(3,1), input_shape=input_shape))\n",
        "model.add(Conv2D(28, kernel_size=(1,3), input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Conv2D(28, kernel_size=(3,3), input_shape=input_shape))\n",
        "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "model.add(Flatten()) # Flattening the 2D arrays for fully connected layers\n",
        "model.add(Dense(128, activation=tf.nn.relu))\n",
        "model.add(Dropout(0.5))      \n",
        "model.add(Dense(10,activation=tf.nn.softmax))\n",
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=20)\n",
        "his = model.fit(x=x_train,y=y_train, epochs=100)\n",
        "_, train_acc = model.evaluate(x_train, y_train, verbose=0)\n",
        "_, test_acc = model.evaluate(x_test, y_test, verbose=0)\n",
        "print('Train: %.3f, Test: %.3f' % (train_acc, test_acc))\n",
        "y_pred = model.predict(x_test)\n",
        "y = np.argmax(y_pred, axis=1)\n",
        "confusion_matrix(y_test, y)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0809 22:46:43.097664 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0809 22:46:43.135907 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0809 22:46:43.144127 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0809 22:46:43.203218 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
            "\n",
            "W0809 22:46:43.254396 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:133: The name tf.placeholder_with_default is deprecated. Please use tf.compat.v1.placeholder_with_default instead.\n",
            "\n",
            "W0809 22:46:43.268444 140712805730176 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "x_train shape: (60000, 28, 28, 1)\n",
            "Number of images in x_train 60000\n",
            "Number of images in x_test 10000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0809 22:46:43.308500 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0809 22:46:43.334270 140712805730176 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3341: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0809 22:46:43.462983 140712805730176 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.2310 - acc: 0.9304\n",
            "Epoch 2/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0961 - acc: 0.9710\n",
            "Epoch 3/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0747 - acc: 0.9773\n",
            "Epoch 4/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0633 - acc: 0.9811\n",
            "Epoch 5/100\n",
            "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0553 - acc: 0.9831\n",
            "Epoch 6/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0505 - acc: 0.9843\n",
            "Epoch 7/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0460 - acc: 0.9850\n",
            "Epoch 8/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0422 - acc: 0.9868\n",
            "Epoch 9/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0411 - acc: 0.9867\n",
            "Epoch 10/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0370 - acc: 0.9883\n",
            "Epoch 11/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0352 - acc: 0.9890\n",
            "Epoch 12/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0348 - acc: 0.9891\n",
            "Epoch 13/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0324 - acc: 0.9898\n",
            "Epoch 14/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0302 - acc: 0.9904\n",
            "Epoch 15/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0294 - acc: 0.9904\n",
            "Epoch 16/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0275 - acc: 0.9911\n",
            "Epoch 17/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0277 - acc: 0.9912\n",
            "Epoch 18/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0266 - acc: 0.9914\n",
            "Epoch 19/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0254 - acc: 0.9919\n",
            "Epoch 20/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0248 - acc: 0.9916\n",
            "Epoch 21/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0248 - acc: 0.9922\n",
            "Epoch 22/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0219 - acc: 0.9927\n",
            "Epoch 23/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0243 - acc: 0.9917\n",
            "Epoch 24/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0197 - acc: 0.9933\n",
            "Epoch 25/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0222 - acc: 0.9927\n",
            "Epoch 26/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0210 - acc: 0.9934\n",
            "Epoch 27/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0211 - acc: 0.9933\n",
            "Epoch 28/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0203 - acc: 0.9933\n",
            "Epoch 29/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0180 - acc: 0.9942\n",
            "Epoch 30/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0195 - acc: 0.9938\n",
            "Epoch 31/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0182 - acc: 0.9939\n",
            "Epoch 32/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0193 - acc: 0.9936\n",
            "Epoch 33/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0194 - acc: 0.9935\n",
            "Epoch 34/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0194 - acc: 0.9936\n",
            "Epoch 35/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0178 - acc: 0.9943\n",
            "Epoch 36/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0186 - acc: 0.9941\n",
            "Epoch 37/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0158 - acc: 0.9944\n",
            "Epoch 38/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0204 - acc: 0.9938\n",
            "Epoch 39/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0158 - acc: 0.9950\n",
            "Epoch 40/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0172 - acc: 0.9944\n",
            "Epoch 41/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0178 - acc: 0.9940\n",
            "Epoch 42/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0138 - acc: 0.9954\n",
            "Epoch 43/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0174 - acc: 0.9944\n",
            "Epoch 44/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0173 - acc: 0.9948\n",
            "Epoch 45/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0163 - acc: 0.9949\n",
            "Epoch 46/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0161 - acc: 0.9951\n",
            "Epoch 47/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0157 - acc: 0.9951\n",
            "Epoch 48/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0158 - acc: 0.9948\n",
            "Epoch 49/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0153 - acc: 0.9953\n",
            "Epoch 50/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0155 - acc: 0.9951\n",
            "Epoch 51/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0156 - acc: 0.9950\n",
            "Epoch 52/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0163 - acc: 0.9950\n",
            "Epoch 53/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0161 - acc: 0.9949\n",
            "Epoch 54/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0138 - acc: 0.9955\n",
            "Epoch 55/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0157 - acc: 0.9952\n",
            "Epoch 56/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0149 - acc: 0.9955\n",
            "Epoch 57/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0150 - acc: 0.9955\n",
            "Epoch 58/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0149 - acc: 0.9954\n",
            "Epoch 59/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0142 - acc: 0.9957\n",
            "Epoch 60/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0150 - acc: 0.9956\n",
            "Epoch 61/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0146 - acc: 0.9954\n",
            "Epoch 62/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0159 - acc: 0.9954\n",
            "Epoch 63/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0141 - acc: 0.9959\n",
            "Epoch 64/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0137 - acc: 0.9958\n",
            "Epoch 65/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0165 - acc: 0.9948\n",
            "Epoch 66/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0162 - acc: 0.9950\n",
            "Epoch 67/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0140 - acc: 0.9959\n",
            "Epoch 68/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0139 - acc: 0.9958\n",
            "Epoch 69/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0158 - acc: 0.9955\n",
            "Epoch 70/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0140 - acc: 0.9959\n",
            "Epoch 71/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0135 - acc: 0.9957\n",
            "Epoch 72/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0158 - acc: 0.9954\n",
            "Epoch 73/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0146 - acc: 0.9959\n",
            "Epoch 74/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0155 - acc: 0.9953\n",
            "Epoch 75/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0141 - acc: 0.9959\n",
            "Epoch 76/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0149 - acc: 0.9954\n",
            "Epoch 77/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0160 - acc: 0.9953\n",
            "Epoch 78/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0129 - acc: 0.9960\n",
            "Epoch 79/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0146 - acc: 0.9958\n",
            "Epoch 80/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0158 - acc: 0.9954\n",
            "Epoch 81/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0139 - acc: 0.9960\n",
            "Epoch 82/100\n",
            "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0149 - acc: 0.9960\n",
            "Epoch 83/100\n",
            "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0137 - acc: 0.9960\n",
            "Epoch 84/100\n",
            "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0139 - acc: 0.9957\n",
            "Epoch 85/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0140 - acc: 0.9958\n",
            "Epoch 86/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0132 - acc: 0.9962\n",
            "Epoch 87/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0125 - acc: 0.9961\n",
            "Epoch 88/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0129 - acc: 0.9965\n",
            "Epoch 89/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0163 - acc: 0.9956\n",
            "Epoch 90/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0143 - acc: 0.9956\n",
            "Epoch 91/100\n",
            "60000/60000 [==============================] - 75s 1ms/step - loss: 0.0124 - acc: 0.9965\n",
            "Epoch 92/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0138 - acc: 0.9963\n",
            "Epoch 93/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0145 - acc: 0.9963\n",
            "Epoch 94/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0130 - acc: 0.9965\n",
            "Epoch 95/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0147 - acc: 0.9961\n",
            "Epoch 96/100\n",
            "60000/60000 [==============================] - 72s 1ms/step - loss: 0.0139 - acc: 0.9958\n",
            "Epoch 97/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0128 - acc: 0.9962\n",
            "Epoch 98/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0149 - acc: 0.9959\n",
            "Epoch 99/100\n",
            "60000/60000 [==============================] - 74s 1ms/step - loss: 0.0146 - acc: 0.9959\n",
            "Epoch 100/100\n",
            "60000/60000 [==============================] - 73s 1ms/step - loss: 0.0139 - acc: 0.9964\n",
            "Train: 1.000, Test: 0.990\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 979,    0,    0,    0,    0,    0,    0,    1,    0,    0],\n",
              "       [   0, 1132,    0,    1,    0,    0,    1,    1,    0,    0],\n",
              "       [   1,    2, 1020,    0,    0,    0,    0,    8,    1,    0],\n",
              "       [   0,    1,    4, 1000,    0,    1,    0,    2,    1,    1],\n",
              "       [   0,    0,    0,    0,  979,    0,    1,    0,    0,    2],\n",
              "       [   1,    0,    0,    6,    0,  882,    1,    1,    0,    1],\n",
              "       [   6,    2,    1,    0,    1,    3,  945,    0,    0,    0],\n",
              "       [   0,    3,    3,    0,    0,    0,    0, 1022,    0,    0],\n",
              "       [   4,    1,    3,    0,    0,    1,    1,    3,  958,    3],\n",
              "       [   1,    3,    0,    0,    5,    6,    0,    9,    3,  982]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    }
  ]
}